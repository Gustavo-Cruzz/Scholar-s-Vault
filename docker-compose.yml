version: '3.8'

services:
  # Optional: Run Qdrant in server mode instead of embedded
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./data/qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334

  # Optional: Run Ollama in Docker
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# To use this docker-compose:
# 1. Make sure Docker and Docker Compose are installed
# 2. For GPU support: Install nvidia-docker2
# 3. Run: docker-compose up -d
# 4. Access Qdrant UI at http://localhost:6333/dashboard
# 5. Ollama will be available at http://localhost:11434

# Note: The default setup uses embedded Qdrant (no Docker needed)
# This is optional for users who prefer containerized services
